# Bert in Production

A small collection of resources on using BERT (https://arxiv.org/abs/1810.04805
) and related Language Models in production environments.

## Descriptive Resources
### Resources on how BERT works.

#### Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)
https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

#### The Illustrated Transformer
http://jalammar.github.io/illustrated-transformer/

#### Sequence-to-sequence modeling with `NN.TRANSFORMER` and `TORCHTEXT`
https://pytorch.org/tutorials/beginner/transformer_tutorial.html

## Implementation Resources
### Resources with implementations of BERT.

#### bert
The original code. TensorFlow code and pre-trained models for BERT.
https://github.com/google-research/bert

#### transformers
ï¿¼Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.
https://github.com/huggingface/transformers

#### fast-bert
Super easy library for BERT based NLP models
https://github.com/kaushaltrivedi/fast-bert

## Papers for Knowledge Distillation of Bert
### Resources on reducing the model size and inference latency by distilling BERT into smaller models, thereby making BERT more production-friendly.

#### Distilling Task-Specific Knowledge from BERT into Simple Neural Networks
https://arxiv.org/abs/1903.12136

#### ALBERT: A Lite BERT for Self-supervised Learning of Language Representations
https://arxiv.org/abs/1909.11942

#### TinyBERT: Distilling BERT for Natural Language Understanding
https://arxiv.org/abs/1909.10351

#### DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter
https://arxiv.org/abs/1910.01108
https://medium.com/huggingface/distilbert-8cf3380435b5
