# Bert in Production

A small collection of resources on using BERT (https://arxiv.org/abs/1810.04805
) and related Language Models in production environments.

## Descriptive Resources - how BERT works.

- [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

- [Sequence-to-sequence modeling with `NN.TRANSFORMER` and `TORCHTEXT`](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)

## Implementation Resources - implementations of BERT.

- [google-research/bert](https://github.com/google-research/bert)
The original code. TensorFlow code and pre-trained models for BERT.

- [huggingface/transformers](https://github.com/huggingface/transformers)
ï¿¼Transformers: State-of-the-art Natural Language Processing for TensorFlow 2.0 and PyTorch.

- [kaushaltrivedi/fast-bert](https://github.com/kaushaltrivedi/fast-bert)
Super easy library for BERT based NLP models

## Papers for Knowledge Distillation of Bert - reduce the model size and inference latency by distilling BERT into smaller models

- [Distilling Task-Specific Knowledge from BERT into Simple Neural Networks](https://arxiv.org/abs/1903.12136)

- [BERT: A Lite BERT for Self-supervised Learning of Language Representations](https://arxiv.org/abs/1909.11942)

- [TinyBERT: Distilling BERT for Natural Language Understanding](https://arxiv.org/abs/1909.10351)

- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter](https://arxiv.org/abs/1910.01108)

- [DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter (article)](https://medium.com/huggingface/distilbert-8cf3380435b5)
